---
slug: "svd7"
title: "Singular Value Decomposition - Properties of Symmetric Matrices 2"
date: 2021-11-12T15:19:57-05:00 
publishdate: 2021-11-14
lastmod: 2021-11-14
tags: ["matrix"]
draft: false
autonumbering: true
output:
  html_document:
    keep_md: true
---

```{r setup, include=FALSE}
# Options to have images saved in the post folder
# And to disable comment symbols before each line of output
knitr::opts_chunk$set(
  message = F,
  fig.path = "",
  comment = "")

# options(digits = 3)

# Knitr hook to use Hugo markdown render image
knitr::knit_hooks$set(plot = function(x,options) {
      base = knitr::opts_knit$get('base.url')
      if (is.null(base)) base = ''
      alt = ifelse (is.null(options$alt),"",options$alt)
      cap = ifelse (is.null(options$caption),"",options$caption)
      if (alt != ""){
        sprintf('![%s](%s%s "%s")', alt, base, x, cap)
      } else {
        sprintf('![%s](%s%s)', cap, base, x)  
        }
  })

# knitr hook to use Hugo highlighting options
knitr::knit_hooks$set(
  source = function(x, options) {
  hlopts <- options$hlopts
    paste0(
      "```r ",
      if (!is.null(hlopts)) {
      paste0("{",
        glue::glue_collapse(
          glue::glue('{names(hlopts)}={hlopts}'),
          sep = ","
        ), "}"
        )
      },
      "\n", glue::glue_collapse(x, sep = "\n"), "\n```\n"
    )
  }
)
```

```{r include=FALSE}
source("../../../static/r/script.R")
```


## Geometrical interpretation of eigendecomposition

In the previous [post]({{< relref "../svd6/index.md" >}}),
we showed how to express a symmetric matrix
as the product of three matrices,
a process known as eigendecomposition.
In this post, we revisit this procedure,
but from a geometrical perspective.

To begin, let's assume that a symmetric matrix $\mathbf{A}$ 
has $n$ eigenvectors,
and each eigenvector $\mathbf{u}_i$ is an $n \times 1$ column vector 

$$
\begin{equation*}
  \mathbf{u}_i = 
  \begin{bmatrix}
    u_{i1} \\
    u_{i2} \\
    \vdots \\
    u_{in} \\
  \end{bmatrix}
\end{equation*}
$$ 

then the transpose of $\mathbf{u}_i$ is a $1 \times n$ row vector

$$
\begin{equation*}
  \mathbf{u}_i^T = 
  \begin{bmatrix}
    u_{i1} & u_{i2} & \ldots & u_{in} \\
  \end{bmatrix}
\end{equation*}
$$ 

and their multiplication

$$
\begin{equation*}
  \mathbf{u}_i\mathbf{u}_i^T = 
  \begin{bmatrix}
    u_{i1} \\
    u_{i2} \\
    \vdots \\
    u_{in} \\
  \end{bmatrix}
  \begin{bmatrix}
    u_{i1} & u_{i2} & \ldots & u_{in} \\
  \end{bmatrix} = 
  \begin{bmatrix}
    u_{i1}u_{i1} & u_{i1}u_{i2} & \ldots & u_{in}u_{in} \\
    u_{i2}u_{i1} & u_{i2}u_{i2} & \ldots & u_{i2}u_{in} \\
    \vdots & \vdots & \vdots & \vdots \\
    u_{in}u_{i1} & u_{in}u_{i2} & \ldots & u_{in}u_{in} \\
  \end{bmatrix}
\end{equation*}
$$ 

becomes an $n \times n$ matrix. 

Let's return to $\mathbf{A} = \mathbf{P}\mathbf{D}\mathbf{P}^\top$. 
First, we calculate $\mathbf{D}\mathbf{P}^\top$ to simplify the eigendecomposition equation:

$$
\begin{align*}
  \begin{bmatrix}
    \lambda_1 & 0 & \ldots & 0 \\
    0 & \lambda_2 & \ldots & 0 \\
    \vdots & \vdots & \vdots & \vdots \\
    0 & 0 & \ldots & \lambda_n \\
  \end{bmatrix}
  \begin{bmatrix}
    \mathbf{u}_1^\top \\
    \mathbf{u}_2^\top \\
    \ldots \\
    \mathbf{u}_n^\top \\
  \end{bmatrix} &= 
  \begin{bmatrix}
    \lambda_1 & 0 & \ldots & 0 \\
    0 & \lambda_2 & \ldots & 0 \\
    \vdots & \vdots & \vdots & \vdots \\
    0 & 0 & \ldots & \lambda_n \\
  \end{bmatrix}
  \begin{bmatrix}
    u_{11} & u_{12} & \ldots & u_{1n} \\
    u_{21} & u_{22} & \ldots & u_{2n} \\
    \vdots & \vdots & \vdots & \vdots \\
    u_{n1} & u_{n2} & \ldots & u_{nn} \\
  \end{bmatrix} \\
  &= 
  \begin{bmatrix}
    \lambda_1u_{11} & \lambda_1u_{12} & \ldots & \lambda_1u_{1n} \\
    \lambda_2u_{21} & \lambda_2u_{22} & \ldots & \lambda_2u_{2n} \\
    \vdots & \vdots & \vdots & \vdots \\
    \lambda_nu_{n1} & \lambda_nu_{n2} & \ldots & \lambda_nu_{nn} \\
  \end{bmatrix} = 
  \begin{bmatrix}
    \lambda_1\mathbf{u}_1^\top \\
    \lambda_2\mathbf{u}_2^\top \\
    \ldots \\
    \lambda_n\mathbf{u}_n^\top \\
  \end{bmatrix}
\end{align*}
$$ 

Now the eigendecomposition becomes:

$$
\begin{equation*}
  \mathbf{A} = 
  \begin{bmatrix}
    \mathbf{u}_1 & \mathbf{u}_2 & \cdots & \mathbf{u}_n \\
  \end{bmatrix}
  \begin{bmatrix}
    \lambda_1\mathbf{u}_1^\top \\
    \lambda_2\mathbf{u}_2^\top \\
    \ldots \\
    \lambda_n\mathbf{u}_n^\top \\
  \end{bmatrix} = 
  \lambda_1\mathbf{u}_1\mathbf{u}_1^\top + \lambda_2\mathbf{u}_2\mathbf{u}_2^\top + \ldots + \lambda_n\mathbf{u}_n\mathbf{u}_n^\top
\end{equation*}
$$ 


Therefore, the $n \times n$ matrix $\mathbf{A}$ can be broken into $n$ matrices 
with the same shape ($n \times n$), 
and each of these matrices has a multiplier 
which is equal to the corresponding eigenvalue $\lambda_i$. 
Each of the matrices $\mathbf{u}_i\mathbf{u}_i^\top$
is called a *projection matrix*.

Imagine that we have a vector $\mathbf{x}$ and a unit vector $\mathbf{v}$. 
The inner product of $\mathbf{v}$ and $\mathbf{x}$ which is equal to 
$\mathbf{v} \cdot \mathbf{x} = \mathbf{v}^\top\mathbf{x}$ gives the scalar projection 
of $\mathbf{x}$ onto $\mathbf{v}$, 
which is the length of the vector projection of $\mathbf{x}$ into $\mathbf{v}$. 
If we multiply $\mathbf{v}^\top\mathbf{x}$ by $\mathbf{v}$ again, 
$\mathbf{v}\mathbf{v}^\top\mathbf{x}$ gives a vector which is called the orthogonal projection of $\mathbf{x}$ onto $\mathbf{v}$.
This is shown in Figure [1](#eigendecomposition-figure).

```{r eigendec, echo=FALSE, results="hide", fig.retina=2, out.width="100%", class="figure", caption="Multiplying a vector with a projection matrix.", alt="eigendecomposition: projecting and scaling (left) then vectorizing (right)."}
vec_x <- c(4, 0.5)
vec_v <- c(1/sqrt(2), 1/sqrt(2))

oldpar <- par(mfrow = c(1, 2))

empty_canvas(
xlim = c(-0.5, 6),
ylim = c (-0.5, 6),
main = expression(paste("Projecting ", bold(x), " onto ", bold(v)))
)

segments(x0 = c(0, 4), y0 = c(0, 0.5), x1 = c(5, 2.25), y1 = c(5, 2.25),
  col = "gray", lty = "dashed", lwd = 2)
matlib::vectors(
  rbind(vec_x, vec_v),
  col = c("red", "blue"),
  lwd = c(2, 2),
  angle = 15,
  labels = c(expression(bold(x)),
    expression(bold(v)))
    )
shape::Arrows(
  x0 = -0.2,
  y0 = 0.2,
  x1 = 2.1,
  y1 = 2.5,
  lwd = 1,
  code = 3,
  col = "dimgrey",
  arr.type = "T")
text(x = 1, y = 2.2, expression(paste(bold(v)^"T", bold(x))), col = "dimgrey")

vec_xv <- vec_x %*% vec_v %*% t(vec_v)

empty_canvas(
xlim = c(-0.5, 6),
ylim = c (-0.5, 6),
main = expression(paste("Vectorizing ", bold(v)^"T", bold(x)))
)

segments(x0 = c(0, 4), y0 = c(0, 0.5), x1 = c(5, 2.25), y1 = c(5, 2.25),
  col = "gray", lty = "dashed", lwd = 2)
matlib::vectors(
  rbind(vec_x, vec_v, vec_xv),
  col = c("red", "blue", "black"),
  lwd = c(2, 2, 2),
  angle = 15,
  labels = c(expression(bold(x)),
    expression(bold(v)),
  expression(paste(bold(v), bold(v)^"T", bold(x))))
    )

par(oldpar)
```


Therefore, when $\mathbf{v}$ is a unit vector, 
multiplying $\mathbf{v}\mathbf{v}^\top$ by $\mathbf{x}$ 
will give the orthogonal projection of $\mathbf{x}$ onto $\mathbf{v}$, 
and that is why $\mathbf{v}\mathbf{v}^\top$ is called the projection matrix. 
Multiplying $\mathbf{u}_i\mathbf{u}_i^\top$ by $\mathbf{x}$, we get the 
orthogonal projection of $\mathbf{x}$ onto $\mathbf{u}_i$. 

Now let's use `R` to calculate the projection matrices of matrix $\mathbf{A}$ 
mentioned before. 

$$
\mathbf{A} = 
\begin{bmatrix}
  3 & 1 \\
  1 & 2 \\
\end{bmatrix}
$$ 

We had already calculated the eigenvalues and eigenvectors of $\mathbf{A}$. 

```{r}
mat_a <- matrix(c(3, 1, 1, 2), 2)
eigen_a <- eigen(mat_a)
eigen_a
```

The next chunk will apply eigendecomposition to $\mathbf{A}$
and print the first term, namely $\mathbf{A}_1$.

```{r eigendecomposition}
u_a <- eigen_a$vectors # an orthogonal matrix made of A's eigenvectors
lambda_a <- eigen_a$values # a vector of A's eigenvalues
mat_a1 <- lambda_a[1] * u_a[,1] %*% t(u_a[,1])
mat_a1 |> round(3)
```

$$
\begin{align*}
  \mathbf{A}_1 &= 
  \lambda_1\mathbf{u}_1\mathbf{u}_1^\top \\
  &= `r lambda_a[1] |> round(3)`
\begin{bmatrix}
  `r u_a[1, 1] |> round(3)` \\
  `r u_a[2, 1] |> round(3)` \\
\end{bmatrix}
\begin{bmatrix}
  `r u_a[1, 1] |> round(3)` & `r u_a[2,1] |> round(3)` \\
\end{bmatrix} =
\begin{bmatrix}
  `r mat_a1[1,1] |> round(3)` & `r mat_a1[1, 2] |> round(3)` \\
  `r mat_a1[2, 1] |> round(3)` & `r mat_a1[2,2] |> round(3)` \\
\end{bmatrix}
\end{align*}
$$ 

As you can see, $\mathbf{A}_1$ is also a symmetric matrix.
In fact, 
it can be shown that 
all projection matrices
$\lambda_i\mathbf{u}_i\mathbf{u}_i^\top$ in the eigendecomposition equation
are symmetric.

Other than being symmetric, projection matrices have some interesting properties.
Let's continue with $\mathbf{A}_1$ as an example.
We can calculate its eigenvalues and eigenvectors:

```{r, results="hold"}
eigen_a1 <- eigen(mat_a1)
cat("eigenvalues: ", "\n")
ifelse(round(eigen_a1$values, 3) < 0.001, 0, round(eigen_a1$values, 3))
cat("eigenvectors: ", "\n")
eigen_a1$vectors |> round(3)
```

$\mathbf{A}_1$ has two eigenvalues.
One is 0.
The other one is equal to $\lambda_1$ of the orignal matrix $\mathbf{A}$.
In addition, its eigenvectors are identical to that of $\mathbf{A}$.
This is not a coincidence.
To see why, suppose we multiple $\mathbf{A}_1$ by $\mathbf{u}_1$:

$$
\begin{equation*}
  \mathbf{A}_1\mathbf{u}_1 =
  \left( \lambda_1\mathbf{u}_1\mathbf{u}_1^\top \right)\mathbf{u}_1 =
  \lambda_1\mathbf{u}_1 \left(\mathbf{u}_1^\top\mathbf{u}_1 \right)
\end{equation*}
$$ 

We know that $\mathbf{u}_1$ is an eigenvector and it is normalized.
Therefore, its length is equal to 1,
so is its inner product with itself.
Thus we have:

$$
\begin{equation*}
  \mathbf{A}_1\mathbf{u}_1 =
  \left( \lambda_1\mathbf{u}_1\mathbf{u}_1^\top \right)\mathbf{u}_1 =
  \lambda_1\mathbf{u}_1
\end{equation*}
$$ 

Thus, $\mathbf{u}_1$ is an eigenvector of $\mathbf{A}_1$,
and the corresponding eigenvalue is $\lambda_1$.

Furthermore,

$$
\begin{equation*}
  \mathbf{A}_1\mathbf{u}_2 =
  \left( \lambda_1\mathbf{u}_1\mathbf{u}_1^\top \right)\mathbf{u}_2 =
  \lambda_1\mathbf{u}_1 \left(\mathbf{u}_1^\top\mathbf{u}_2 \right)
\end{equation*}
$$ 

Because $\mathbf{A}$ is symmetric,
its eigenvectors $\mathbf{u}_1$ and $\mathbf{u}_2$ are orthogonal, 
or perpendicular.
Given that the inner product of two perpendicular vectors is zero,
the inner product of $\mathbf{u}_1$ and $\mathbf{u}_2$ is zero.
Thus we have

$$
\begin{equation*}
  \mathbf{A}_1\mathbf{u}_2 =
  \left( \lambda_1\mathbf{u}_1\mathbf{u}_1^\top \right)\mathbf{u}_2 =
  \lambda_1\mathbf{u}_1 \left(\mathbf{u}_1^\top\mathbf{u}_2 \right)
\end{equation*} = 0 = 0 \times \mathbf{u}_2
$$ 

which means that $\mathbf{u}_2$ is also an eigenvector of $\mathbf{A}_1$
and its corresponding eigenvalue is 0,
matching the output of `eigen_a1$values[2]` = `r eigen_a1$values[2] |> round(0)`.


In general,
eigendecomposition decomposes a symmetric matrix
into $n$ of $n \times n$ projection matrices,
$\lambda_i\mathbf{u}_i\mathbf{u}_i^\top$.

Each projection matrix is also symmetric,
which shares the same eigenvectors as the original matrix.
For a particular project matrix $\lambda_k\mathbf{u}_k\mathbf{u}_k^\top$,
the corresponding eigenvalue of eigenvector $\mathbf{u}_k$ 
is the $k$-th eigenvalue of $\mathbf{A}$, $\lambda_k$,
whereas all the remaining eigenvalues are zero.

Recall that a symmetric matrix scales a vector
along its eigenvectors,
proportionally to the corresponding eigenvalue.
Therefore, a projection matrix $\lambda_i\mathbf{u}_i\mathbf{u}_i^\top$
stretches/shrinks a vector along $\mathbf{u}_i$ by $\lambda_i$,
but shrinks the vector to zero in all other directions.
Let's illustrate this with $\mathbf{A}$
and one of its projection matrix $\mathbf{A}_1$ in Figure [2](#projection-figure).

```{r projection-matrix, echo=FALSE, results="hide", class="figure", caption="Original vectors (left) and transformed vectors by a projection matrix (right).", alt="projection: Original vectors (left) and transformed vectors (right)."}
oldpar <- par(mfrow = c(1, 2))

empty_canvas()
unit_circle() |> plot_unit_circle()

empty_canvas()
transform(mat_a1) |> plot_transformed()
matlib::vectors(rbind(
    eigen_a1$vectors[, 1],
    eigen_a1$vectors[, 2]),
  labels = NULL
  )
text(c(-1, 1), c(-1, -1), c(expression(bold(u[1])), expression(bold(u[2]))))
text(x = 2, y = 3, expression(paste(
      lambda[1],
      bold(u[1]),
      bold(u[1])^"T",
      bold(x)
      )),
    col = "#56B4E9",
    cex = 1.5
)
par(oldpar)
```

All vectors in $\mathbb{X}$ are transformed by $\mathbf{A}_1$,
namely, stretched along $\mathbf{u}_1$ and shrunk to zero along $\mathbf{u}_2$.
As a result, the initial circle became a straight line.
<!-- extending from xx to xx. -->

Previously, matrix $\mathbf{A}$ transformed vectors in $\mathbb{X}$
into an ellipse, another 2-D shape.
And yet, matrix $\mathbf{A}_1$ transformed vectors in $\mathbb{X}$
into a line, a 1-D shape.
Both $\mathbf{A}$ and $\mathbf{A}_1$ are symmetric.
How come one preserves whereas the other reduces the dimension?
In the [next post]({{< relref "../svd8/index.md" >}}),
we will discuss the reason by introducing the concept of *rank*.


